@misc{schulhoff_showing_2024,
	title = {Showing {Examples}},
	url = {https://learnprompting.org/docs/basics/few_shot},
	journal = {Showing Examples},
	author = {Schulhoff, Sander},
	month = jul,
	year = {2024},
}

@misc{neuhaus_canvasxpress_nodate,
	title = {{CanvasXpress}: {A} {JavaScript} {Library} for {Data} {Analytics} with {Full} {Audit} {Trail} {Capabilities}},
	url = {https://www.canvasxpress.org/},
	author = {Neuhaus, Isaac},
}

@misc{noauthor_better_nodate,
	title = {Better language models and their implications},
	url = {https://openai.com/index/better-language-models/},
	abstract = {We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.},
	language = {en-US},
	urldate = {2024-08-21},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\VBA856TC\\better-language-models.html:text/html},
}

@misc{gao_retrieval-augmented_2023,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2312.10997v5},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	language = {en},
	urldate = {2024-08-21},
	journal = {arXiv.org},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = dec,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\smitha26\\Zotero\\storage\\YVVAXCAB\\Gao et al. - 2023 - Retrieval-Augmented Generation for Large Language .pdf:application/pdf},
}

@article{li2024longcontextllmsstrugglelong,
      title={Long-context LLMs Struggle with Long In-context Learning}, 
      author={Tianle Li and Ge Zhang and Quy Duc Do and Xiang Yue and Wenhu Chen},
      year={2024},
      eprint={2404.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.02060}, 
}

@article{chen_bge_2024,
	title = {{BGE} {M3}-{Embedding}: {Multi}-{Lingual}, {Multi}-{Functionality}, {Multi}-{Granularity} {Text} {Embeddings} {Through} {Self}-{Knowledge} {Distillation}},
	shorttitle = {{BGE} {M3}-{Embedding}},
	url = {http://arxiv.org/abs/2402.03216},
	doi = {10.48550/arXiv.2402.03216},
	abstract = {In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
	month = jun,
	year = {2024},
	note = {arXiv:2402.03216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\smitha26\\Zotero\\storage\\VMAVJ6HA\\Chen et al. - 2024 - BGE M3-Embedding Multi-Lingual, Multi-Functionali.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\43YTCHMN\\2402.html:text/html},
}

@misc{noauthor_milvus-iopymilvus_2024,
	title = {milvus-io/pymilvus},
	copyright = {Apache-2.0},
	url = {https://github.com/milvus-io/pymilvus},
	abstract = {Python SDK for Milvus.},
	urldate = {2024-08-21},
	publisher = {The Milvus Project},
	month = aug,
	year = {2024},
	note = {original-date: 2019-06-13T11:38:34Z},
	keywords = {python, anns, database, hacktoberfest, milvus, sdk, vector},
}

@inproceedings{khattab_colbert_2020,
	address = {New York, NY, USA},
	series = {{SIGIR} '20},
	title = {{ColBERT}: {Efficient} and {Effective} {Passage} {Search} via {Contextualized} {Late} {Interaction} over {BERT}},
	isbn = {978-1-4503-8016-4},
	shorttitle = {{ColBERT}},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401075},
	doi = {10.1145/3397271.3401075},
	abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.},
	urldate = {2024-08-21},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Khattab, Omar and Zaharia, Matei},
	month = jul,
	year = {2020},
	pages = {39--48},
	file = {Full Text PDF:C\:\\Users\\smitha26\\Zotero\\storage\\FNKUUPUV\\Khattab and Zaharia - 2020 - ColBERT Efficient and Effective Passage Search vi.pdf:application/pdf},
}

@misc{bobbitt_quick_2020,
	title = {A {Quick} {Intro} to {Leave}-{One}-{Out} {Cross}-{Validation} ({LOOCV})},
	url = {https://www.statology.org/leave-one-out-cross-validation/},
	abstract = {This tutorial provides a quick introduction to leave-one-out cross validation, a commonly used method in machine learning.},
	language = {en-US},
	urldate = {2024-08-21},
	journal = {Statology},
	author = {Bobbitt, Zach},
	month = nov,
	year = {2020},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\4WKARJE2\\leave-one-out-cross-validation.html:text/html},
}

@misc{noauthor_hello_nodate,
	title = {Hello {GPT}-4o {\textbar} {OpenAI}},
	url = {https://openai.com/index/hello-gpt-4o/},
	urldate = {2024-08-21},
}

@article{dubey_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
	month = aug,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\smitha26\\Zotero\\storage\\2AC2TIYA\\Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\8CFWCU39\\2407.html:text/html},
}

@misc{noauthor_meet_nodate,
	title = {Meet {Claude}},
	url = {https://www.anthropic.com/claude},
	abstract = {Claude is AI for all of us. Whether you're brainstorming alone or building with a team of thousands, Claude is here to help.},
	language = {en},
	urldate = {2024-08-21},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\CPZ33X6S\\claude.html:text/html},
}

@misc{ai_large_2024,
	title = {Large {Enough}},
	url = {https://mistral.ai/news/mistral-large-2407/},
	abstract = {Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.},
	language = {en-us},
	urldate = {2024-08-21},
	author = {AI, Mistral},
	month = jul,
	year = {2024},
	note = {Section: news},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\98U7TVM3\\mistral-large-2407.html:text/html},
}

@misc{noauthor_google_nodate,
	title = {Google {AI} {Gemini} {API} {\textbar} {Google} {AI} {Studio} {\textbar} {Google} for {Developers}},
	url = {https://ai.google.dev/gemini-api},
	abstract = {Get started with the Gemini API in Google AI Studio. Get a Gemini API key, quickly develop prompts, and build with Gemini 1.5 Flash and 1.5 Pro.},
	language = {en},
	urldate = {2024-08-21},
	journal = {Google AI for Developers},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\85EB9AFM\\gemini-api.html:text/html},
}

@misc{noauthor_phi-3_nodate,
	title = {Phi-3 {Open} {Models} - {Small} {Language} {Models} {\textbar} {Microsoft} {Azure}},
	url = {https://azure.microsoft.com/en-us/products/phi-3},
	abstract = {Explore Phi-3 models, efficient small language models (SLMs) for generative AI applications. Learn more about Phi-3 in Azure AI Studio.},
	language = {en-US},
	urldate = {2024-08-21},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\XANU2B69\\phi-3.html:text/html},
}

@misc{noauthor_introducing_2024,
	title = {Introducing {DBRX}: {A} {New} {State}-of-the-{Art} {Open} {LLM}},
	shorttitle = {Introducing {DBRX}},
	url = {https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
	language = {en-US},
	urldate = {2024-08-21},
	journal = {Databricks},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\SUZGFFZR\\introducing-dbrx-new-state-art-open-llm.html:text/html},
}

@misc{noauthor_gpt-4_nodate,
	title = {{GPT}-4},
	url = {https://openai.com/index/gpt-4/},
	abstract = {It can generate, edit, and iterate with users on creative and technical writing tasks, such as composing songs, writing screenplays, or learning a user’s writing style.},
	language = {en-US},
	urldate = {2024-08-21},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\CT7L55H3\\gpt-4.html:text/html},
}

@misc{borgne_openai_2024,
	title = {{OpenAI} vs {Open}-{Source} {Multilingual} {Embedding} {Models}},
	url = {https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05},
	abstract = {Choosing the model that works best for your data},
	language = {en},
	urldate = {2024-08-26},
	journal = {Medium},
	author = {Borgne, Yann-Aël Le},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\VG7TPK49\\openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05.html:text/html},
}

@misc{noauthor_canvasxpress_nodate,
	title = {{CanvasXpress}: {AI} using {LLM}},
	shorttitle = {{CanvasXpress}},
	url = {https://www.canvasxpress.org/llm.html},
	abstract = {CanvasXpress: CanvasXpress AI. A new way to explore data with machine learning and artificial intelligence. Generate code and visualizations with a single click.},
	language = {en},
	urldate = {2024-08-26},
	journal = {CanvasXpress},
	file = {Snapshot:C\:\\Users\\smitha26\\Zotero\\storage\\ZF6Y3C9U\\llm.html:text/html},
}

@inproceedings{tennant_menu-based_1984,
	title = {Menu-based natural language understanding},
	url = {https://www.computer.org/csdl/proceedings-article/afips/1984/50910629/12OmNwcUjYE},
	doi = {10.1109/AFIPS.1984.52},
	language = {English},
	urldate = {2025-01-08},
	publisher = {IEEE Computer Society},
	author = {Tennant, Harry},
	month = dec,
	year = {1984},
	pages = {629--629},
}
